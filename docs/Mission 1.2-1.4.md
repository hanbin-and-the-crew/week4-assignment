### **1.2 네트워크 아키텍처 설계**

**핵심 결정 사항**

1. **VPC CIDR 범위**
    - 10.0.0.0/16을 사용하면 65,536개의 IP 주소 사용 가능하고 10.0.0.0/24를 사용하면 256개의 IP 주소 사용 가능하다.
    - 평상시에 동시 사용자는 100명으로 이는 10.0.0.0/24 CIDR을 이용하여도 충분향 사용자량이다.
    - 하지만 블랙프라이데이의 경우 동시 사용자 1000명까지 고려해야하며, Public, Private Subnet이 각각 2개씩 존재하므로 500명씩 나뉜다 하더라도 이는 10.0.0.0/24로는 무리가 존재한다.
    - 또한, 고객사 10개까지 확장을 고려한다면 1000~5000명의 사용자를 버틸 수 있는 시스템이 되어야 하며 CIDR 역시 넓게 잡는 것이 유리하다.
    - 따라서 IPv4 CIDR은 `10.0.0.0/16`을 이용하였다.
2. **Subnet 구성**
    
    
    | Subnet 이름 | AZ | CIDR | 유형 | 용도 |
    | --- | --- | --- | --- | --- |
    | **public-subnet-2a** | ap-northeast-2a | 10.0.1.0/24 | Public | ALB, Bastion |
    | **public-subnet-2c** | ap-northeast-2c | 10.0.2.0/24 | Public | ALB |
    | **private-app-subnet-2a** | ap-northeast-2a | 10.0.11.0/24 | Private | ECS Task |
    | **private-app-subnet-2c** | ap-northeast-2c | 10.0.12.0/24 | Private | ECS Task |
    | **db-subnet-2a** | ap-northeast-2a | 10.0.21.0/24 | DB | RDS |
    | **db-subnet-2c** | ap-northeast-2c | 10.0.22.0/24 | DB | RDS |
3. **Route Table 설정**
    - DB Route Table은 **절대 인터넷 통신 없음**
    - Public Subnet만 IGW 연결
    - Private Subnet은 NAT로 Outbound만 허용
    
    | Route Table | 용도 | Subnet 연결 | 라우팅 규칙 |
    | --- | --- | --- | --- |
    | **rt-public** | Public | public-subnet-2a, 2c | 0.0.0.0/0 → IGW |
    | **rt-private-a** | Private App | private-app-subnet-2a | 0.0.0.0/0 → natgw-2a |
    | **rt-private-c** | Private App | private-app-subnet-2c | 0.0.0.0/0 → natgw-2c |
    | **rt-db** | DB 전용 | db-subnet-2a, 2c | local only |

### **1.3 데이터베이스 설계 결정**

**핵심 질문**

1. **Single-AZ vs Multi-AZ**
    - 저희 시스템의 경우 DB 상에서 모니터링을 위한 Cloudwatch도 이용하기 때문에 비용이 추가로 발생합니다. 따라서 Single-AZ와 Multi-AZ가 스토리지 금액을 제하고는 월 약 $30의 비용 차이가 발생합니다. 2배의 비용 차이지만 저희 시스템 초기 예산안에는 매우 크게 영향을 주기 때문에 예산안에 부합하기 위해 Single-AZ를 이용하였습니다.
    - 물론 이는 가용 영역 장애 시 수동으로 복구해야 하며, Multi-AZ를 쓸 때 자동 Failover보다 매우 오래 걸리기 때문에 초창기에는 어려움을 겪을 것으로 예상됩니다. 대신 시스템이 점점 커지고 예산안이 늘어날 경우 제일 먼저 자동 복구를 보장하기 위해서 RDS를 Multi-AZ를 사용하게 만드는 것이 저희의 우선순위입니다.
2. **인스턴스 타입**
    - 일단, 시작은 db.t3.micro 0.5vCPU 1GB RAM을 이용하였습니다. 평상시 평균 트래픽을 처리하는 데는 충분하며, 예산안이 빠듯하기 때문에 예산에 큰 부분을 차지하는 DB에서 비용을 줄이고자 하였습니다. 물론, 블랙프라이데이의 경우가 존재하므로 일단은 이런 식으로 사용하다가 Scale Up이나 Scale Out을 이용하여 DB를 성능적으로도, 수적으로도 늘려갈 생각입니다.
    - 이후 고객사 10개까지 확장되는 시나리오의 경우 AWS RDS의 인스턴스를 더 좋은 모델로 Scale Up하여 서비스할 것입니다.
3. **백업 전략**
    - 백업의 경우 DB 크기만큼 무료, 초과분만 과금이므로 최대한 예산안에 맞추기 위해 비용이 발생하지 않는 선에서 설정하였습니다. 대신 서비스가 이후 확장될 경우에는 자동 백업 보관 기관도 늘릴 것이며, 분산 DB까지도 고려할 수 있으므로 더욱 예산이 나갈 수 있습니다.

---

### **1.4 컨테이너 및 배포 전략**

**Docker 이미지 설계**

- Base Image는 우리 프로젝트의 모듈 단위 크기가 매우 작은 편이기 때문에 아주 작고 가벼운 경량형 모델인 eclipse-temurin:17-jre-alpine을 사용하였다.
- 이미지 크기 최적화의 경우 .dockerignore 등을 이용할 순 있으나 아직 파일이 매우 작기 때문에 따로 사용하진 않았다. 물론 서비스 규모가 커지고 코드 및 파일의 크기가 커질 경우 도입한다.
- Multi-stage build는 아직 효과도 크지 않으므로 따로 사용하지 않았지만, 역시 이미지 크기가 커질 경우 크기를 줄이기 위해서 도입한다.

**ECS 배포 전략**

- 우리 서비스의 경우 업무 시간에는 동시 사용자가 100명 + 20req/s, 점심시간에는 20명 + 5req/s, 그외 야간의 경우 < 1req/s로 규모가 크지 않다.
- 따라서 블랙프라이데이 같은 특수한 경우가 아닌 이상 하나의 Task를 새 버전으로 교체하는데 크게 부담이 가지 않는다. 또한 시스템 초창기이므로 배포가 자주 발생할 것으로 예상된다.
- **Rolling Update**의 경우 간단하고, 추가 리소스가 불필요하며, 비용 효율적인 방식이라 적합하다고 생각한다. 특히 빈번한 배포 환경에서는 가장 유용한 전략이다.
- 비교군인 **Blue-Green Deployment**의 경우 배포 중 ECS를 2배로 늘리는 만큼 롤백도 간편하고, 금융 도메인같이 아주 중요한 배포에서는 유용한 방식이지만, 우리 프로젝트에서는 이 정도의 자원을 투자할만큼의 필요는 없다고 생각하였다.
- 따라서 **Rolling Update** 방식으로 SwiftLogix를 배포할 것이다.
